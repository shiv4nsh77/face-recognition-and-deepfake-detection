{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66304760-a234-40fd-bf7b-0fefe5558981",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install face_recognition efficientnet tensorflow opencv-python-headless gradio --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94aa6a2-0f81-4ec7-ac5c-22bcfcc9a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import gradio as gr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c225f18b-1cac-4233-8348-73be74c25164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def load_images(folder, label, size=(244, 244)):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder):\n",
    "        path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, size)\n",
    "            data.append((img, label))\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "real_path = r\"D:\\online_set\\real_and_fake_face\\real\"\n",
    "fake_path = r\"D:\\online_set\\real_and_fake_face\\fake\"\n",
    "\n",
    "real_data = load_images(real_path, 0)\n",
    "fake_data = load_images(fake_path, 1)\n",
    "data = real_data + fake_data\n",
    "\n",
    "# Shuffle\n",
    "random.shuffle(data)\n",
    "\n",
    "# Convert to NumPy with reduced memory usage\n",
    "X = np.array([item[0] for item in data], dtype=np.float32) / 255.0\n",
    "y = np.array([item[1] for item in data])\n",
    "y = to_categorical(y, num_classes=2)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903579af-6f9e-4c6f-a587-a5272a34adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 1s/step - accuracy: 0.5005 - loss: 0.7172 - val_accuracy: 0.4775 - val_loss: 0.7097\n",
      "Epoch 2/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.4972 - loss: 0.7054 - val_accuracy: 0.4775 - val_loss: 0.7004\n",
      "Epoch 3/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.5031 - loss: 0.7102 - val_accuracy: 0.5225 - val_loss: 0.6940\n",
      "Epoch 4/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.4753 - loss: 0.7197 - val_accuracy: 0.5201 - val_loss: 0.6918\n",
      "Epoch 5/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.4788 - loss: 0.7126 - val_accuracy: 0.4775 - val_loss: 0.6958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(244, 244, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=16),\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "model.save(\"deepfake_detector_efficientnetb4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963ae29d-6ab8-4388-9c8c-d17ae89e41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.5009 - loss: 0.6999 - val_accuracy: 0.5177 - val_loss: 0.6916\n",
      "Epoch 2/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 1s/step - accuracy: 0.5275 - loss: 0.6899 - val_accuracy: 0.5225 - val_loss: 0.6947\n",
      "Epoch 3/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 1s/step - accuracy: 0.5043 - loss: 0.6950 - val_accuracy: 0.5083 - val_loss: 0.6911\n",
      "Epoch 4/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 1s/step - accuracy: 0.5191 - loss: 0.6938 - val_accuracy: 0.5059 - val_loss: 0.6916\n",
      "Epoch 5/5\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 1s/step - accuracy: 0.5051 - loss: 0.6947 - val_accuracy: 0.5296 - val_loss: 0.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Step 4: Recompile with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Continue training (fine-tuning)\n",
    "model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=16),\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Step 6: Save the fine-tuned model\n",
    "model.save(\"deepfake_detector_efficientnetb4_finetuned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9902c00-4c8a-41c7-912e-ce81a220bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face found in: elon musk100_1525.jpg\n",
      "No face found in: elon musk114_1535.jpg\n",
      "No face found in: elon musk145_1549.jpg\n",
      "No face found in: elon musk147_1551.jpg\n",
      "No face found in: elon musk209_1587.jpg\n",
      "No face found in: elon musk53_1631.jpg\n",
      "No face found in: jeff bezos163_2070.jpg\n",
      "No face found in: Taylor Swift119_4562.jpg\n",
      "No face found in: Taylor Swift144_4572.jpg\n",
      "No face found in: Taylor Swift189_4606.jpg\n",
      "No face found in: Taylor Swift195_4607.jpg\n",
      "No face found in: Taylor Swift212_4617.jpg\n",
      "No face found in: Tom Cruise124_1046.jpg\n",
      "No face found in: Tom Cruise133_1053.jpg\n",
      "No face found in: Tom Cruise146_1061.jpg\n",
      "No face found in: Tom Cruise15_1065.jpg\n",
      "No face found in: Tom Cruise217_1123.jpg\n",
      "No face found in: Tom Cruise246_1144.jpg\n",
      "No face found in: Tom Cruise63_1177.jpg\n",
      "No face found in: Tom Cruise80_1193.jpg\n",
      "No face found in: Tom Cruise80_1194.jpg\n",
      "No face found in: Tom Cruise86_1201.jpg\n",
      "No face found in: tom ellis106_4255.jpg\n",
      "No face found in: tom ellis133_4279.jpg\n",
      "No face found in: tom ellis138_4282.jpg\n",
      "No face found in: tom ellis178_4315.jpg\n",
      "No face found in: tom ellis192_4325.jpg\n",
      "No face found in: tom ellis207_4337.jpg\n",
      "No face found in: tom ellis216_4347.jpg\n",
      "No face found in: tom ellis216_4348.jpg\n",
      "No face found in: tom ellis3_4376.jpg\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import face_recognition\n",
    "\n",
    "known_faces = []\n",
    "known_names = []\n",
    "\n",
    "dataset_path = Path(r\"D:\\new_project\\face_dataset\")\n",
    "\n",
    "# Accept common image file extensions\n",
    "valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "\n",
    "for img_path in dataset_path.rglob(\"*\"):\n",
    "    if img_path.suffix.lower() not in valid_extensions:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        image = face_recognition.load_image_file(str(img_path))\n",
    "        encodings = face_recognition.face_encodings(image)\n",
    "\n",
    "        if encodings:\n",
    "            known_faces.append(encodings[0])\n",
    "            name = img_path.parent.name  # Use folder name as label\n",
    "            known_names.append(name)\n",
    "        else:\n",
    "            print(f\"No face found in: {img_path.name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "186d9779-8038-4050-b8df-3724b60e675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your trained model before prediction\n",
    "model = load_model(\"deepfake_detector_efficientnetb4_finetuned.h5\")\n",
    "\n",
    "def predict_and_recognize(image, is_webcam=False):\n",
    "    try:\n",
    "        # Deepfake Detection (skip if webcam input)\n",
    "        if not is_webcam:\n",
    "            image_resized = cv2.resize(image, (244, 244)) / 255.0\n",
    "            input_tensor = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "            pred = model.predict(input_tensor)[0]\n",
    "            predicted_class = np.argmax(pred)\n",
    "            confidence = round(float(np.max(pred)) * 100, 2)\n",
    "\n",
    "            is_real = predicted_class == 0 and confidence >= 51.0\n",
    "            result = \"Real\" if is_real else \"Fake\"\n",
    "            full_result = f\"{result} Face ({confidence}% Confidence)\"\n",
    "        else:\n",
    "            # Force real for webcam\n",
    "            full_result = \"Real Face (Captured via Webcam)\"\n",
    "\n",
    "        # Face recognition (always performed)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        face_locations = face_recognition.face_locations(rgb_image)\n",
    "        if not face_locations:\n",
    "            return f\"{full_result} - No face detected for recognition\"\n",
    "\n",
    "        face_encodings = face_recognition.face_encodings(rgb_image, face_locations)\n",
    "        if not face_encodings:\n",
    "            return f\"{full_result} - No encodings could be computed\"\n",
    "\n",
    "        for face_encoding in face_encodings:\n",
    "            matches = face_recognition.compare_faces(known_faces, face_encoding)\n",
    "            face_distances = face_recognition.face_distance(known_faces, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "\n",
    "            if matches[best_match_index]:\n",
    "                name = known_names[best_match_index]\n",
    "                return f\"{full_result} - Identified as: {name}\"\n",
    "\n",
    "        return f\"{full_result} - Face Not Recognized\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during prediction: {str(e)}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "716fc826-6497-49fb-85b7-f5f7c207d839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface = gr.Interface(fn=predict_and_recognize,\n",
    "                         inputs=gr.Image(type=\"numpy\"),\n",
    "                         outputs=\"text\",\n",
    "                         title=\"Deepfake Detection + Face Recognition\",\n",
    "                         description=\"Upload a face image. It will classify as real/fake and identify the person if real.\")\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d802d-2639-46b1-b462-c1aa7bbec976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ana]",
   "language": "python",
   "name": "conda-env-ana-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
